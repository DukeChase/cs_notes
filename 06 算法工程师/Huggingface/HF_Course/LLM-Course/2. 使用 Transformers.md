# [æœ¬ç« ç®€ä»‹](https://huggingface.co/learn/llm-course/zh-CN/chapter2/1)

ğŸ¤— Transformers åº“åº”è¿è€Œç”Ÿï¼Œå°±æ˜¯ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å®ƒçš„ç›®æ ‡æ˜¯æä¾›ä¸€ä¸ªç»Ÿä¸€çš„ API æ¥å£ï¼Œé€šè¿‡å®ƒå¯ä»¥åŠ è½½ã€è®­ç»ƒå’Œä¿å­˜ä»»ä½• Transformer æ¨¡å‹ã€‚è¯¥åº“çš„ä¸»è¦ç‰¹ç‚¹æœ‰ï¼š

- **æ˜“äºä½¿ç”¨**ï¼šä»…éœ€ä¸¤è¡Œä»£ç ï¼Œå°±èƒ½ä¸‹è½½ã€åŠ è½½å¹¶ä½¿ç”¨å…ˆè¿›çš„ NLP æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚
- **çµæ´»**ï¼šåœ¨æœ¬è´¨ä¸Šï¼Œæ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯ç®€å•çš„ PyTorch nn.Module æˆ– TensorFlow tf.keras.Model ç±»ï¼Œå¹¶å¯åƒåœ¨å„è‡ªçš„æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¡†æ¶ä¸­å¤„ç†å…¶ä»–æ¨¡å‹ä¸€æ ·å¤„ç†å®ƒä»¬ã€‚
- **ç®€å•**ï¼šè¯¥åº“å‡ ä¹æ²¡æœ‰è¿›è¡Œä»»ä½•æŠ½è±¡åŒ–ã€‚ğŸ¤— Transformers åº“ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µæ˜¯â€œå…¨åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­â€ï¼šæ¨¡å‹çš„å‰å‘ä¼ æ’­å®Œå…¨åœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­å®šä¹‰ï¼Œè¿™ä½¿å¾—ä»£ç æœ¬èº«æ˜“äºç†è§£å’Œä¿®æ”¹ã€‚

# ç®¡é“çš„å†…éƒ¨

![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg)

## ä½¿ç”¨ tokenizerï¼ˆ tokenizer ï¼‰è¿›è¡Œé¢„å¤„ç†

ä¸å…¶ä»–ç¥ç»ç½‘ç»œä¸€æ ·ï¼ŒTransformer æ¨¡å‹æ— æ³•ç›´æ¥å¤„ç†åŸå§‹æ–‡æœ¬ï¼Œå› æ­¤æˆ‘ä»¬ç®¡é“çš„ç¬¬ä¸€æ­¥æ˜¯å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ tokenizerï¼ˆ tokenizer ï¼‰ï¼Œå®ƒå°†è´Ÿè´£ï¼š

- å°†è¾“å…¥æ‹†åˆ†ä¸ºå•è¯ã€å­å•è¯æˆ–ç¬¦å·ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œç§°ä¸ºÂ **token**ï¼ˆæ ‡è®°ï¼‰
- å°†æ¯ä¸ªæ ‡è®°ï¼ˆtokenï¼‰æ˜ å°„åˆ°ä¸€ä¸ªæ•°å­—ï¼Œç§°ä¸ºÂ **input ID**ï¼ˆinputs IDï¼‰
- æ·»åŠ æ¨¡å‹éœ€è¦çš„å…¶ä»–è¾“å…¥ï¼Œä¾‹å¦‚ç‰¹æ®Šæ ‡è®°ï¼ˆå¦‚Â `[CLS]`Â å’ŒÂ `[SEP]`Â ï¼‰
	- ä½ç½®ç¼–ç ï¼šæŒ‡ç¤ºæ¯ä¸ªæ ‡è®°åœ¨å¥å­ä¸­çš„ä½ç½®ã€‚
	- æ®µè½æ ‡è®°ï¼šåŒºåˆ†ä¸åŒæ®µè½çš„æ–‡æœ¬ã€‚
	- ç‰¹æ®Šæ ‡è®°ï¼šä¾‹å¦‚ [CLS] å’Œ [SEP] æ ‡è®°ï¼Œç”¨äºæ ‡è¯†å¥å­çš„å¼€å¤´å’Œç»“å°¾ã€‚

## æ¢ç´¢æ¨¡å‹
### é«˜ç»´å‘é‡
Transformers æ¨¡å—çš„çŸ¢é‡è¾“å‡ºé€šå¸¸è¾ƒå¤§ã€‚å®ƒé€šå¸¸æœ‰ä¸‰ä¸ªç»´åº¦ï¼š
- **Batch size**ï¼ˆæ‰¹æ¬¡å¤§å°ï¼‰ï¼šä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 2ï¼‰ã€‚
- **Sequence length**ï¼ˆåºåˆ—é•¿åº¦ï¼‰ï¼šè¡¨ç¤ºåºåˆ—ï¼ˆå¥å­ï¼‰çš„é•¿åº¦ï¼ˆåœ¨æˆ‘ä»¬çš„ç¤ºä¾‹ä¸­ä¸º 16ï¼‰ã€‚
- **Hidden size**ï¼ˆéšè—å±‚å¤§å°ï¼‰ï¼šæ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´åº¦ã€‚

### æ¨¡å‹å¤´ï¼šç†è§£æ•°å­—çš„æ„ä¹‰
## å¯¹è¾“å‡ºè¿›è¡Œååºå¤„ç†

# [æ¨¡å‹](https://huggingface.co/learn/llm-course/zh-CN/chapter2/3)

## åˆ›å»º Transformer æ¨¡å‹

åˆå§‹åŒ– BERT æ¨¡å‹éœ€è¦åšçš„ç¬¬ä¸€ä»¶äº‹æ˜¯åŠ è½½Â `Config`Â å¯¹è±¡ï¼š
```python
from transformers import BertConfig, BertModel

# åˆå§‹åŒ– Config ç±»
config = BertConfig()

# ä» Config ç±»åˆå§‹åŒ–æ¨¡å‹
model = BertModel(config)
```
### ä½¿ç”¨ä¸åŒçš„åŠ è½½æ–¹å¼
ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºæ¨¡å‹ä¼šä½¿ç”¨éšæœºå€¼å¯¹å…¶è¿›è¡Œåˆå§‹åŒ–ï¼š

```python
from transformers import BertConfig, BertModel

config = BertConfig()
model = BertModel(config)

# æ¨¡å‹å·²éšæœºåˆå§‹åŒ–!
```

è¿™ä¸ªæ¨¡å‹æ˜¯å¯ä»¥è¿è¡Œå¹¶å¾—åˆ°ç»“æœçš„ï¼Œä½†å®ƒä¼šè¾“å‡ºèƒ¡è¨€ä¹±è¯­ï¼›å®ƒéœ€è¦å…ˆè¿›è¡Œè®­ç»ƒæ‰èƒ½æ­£å¸¸ä½¿ç”¨ã€‚æˆ‘ä»¬å¯ä»¥æ ¹æ®æ‰‹å¤´çš„ä»»åŠ¡ä»å¤´å¼€å§‹è®­ç»ƒæ¨¡å‹ï¼Œä½†æ­£å¦‚ä½ åœ¨Â [ç¬¬ä¸€ç« ](https://huggingface.co/course/chapter1)Â ä¸­çœ‹åˆ°çš„ï¼Œè¿™å°†éœ€è¦å¾ˆé•¿çš„æ—¶é—´å’Œå¤§é‡çš„æ•°æ®ï¼Œå¹¶ä¸”äº§ç”Ÿçš„ç¢³è¶³è¿¹ä¼šå¯¹ç¯å¢ƒäº§ç”Ÿä¸å¯å¿½è§†çš„å½±å“ã€‚ä¸ºäº†é¿å…ä¸å¿…è¦çš„é‡å¤å·¥ä½œï¼Œèƒ½å¤Ÿå…±äº«å’Œå¤ç”¨å·²ç»è®­ç»ƒè¿‡çš„æ¨¡å‹æ˜¯éå¸¸é‡è¦çš„ã€‚

åŠ è½½å·²ç»è®­ç»ƒè¿‡çš„ Transformers æ¨¡å‹å¾ˆç®€å•â€”â€”æˆ‘ä»¬å¯ä»¥ä½¿ç”¨Â `from_pretrained()`Â æ–¹æ³•ï¼š

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")
```

ç°åœ¨ï¼Œæ­¤æ¨¡å‹å·²ç»ç”¨ checkpoint çš„æ‰€æœ‰æƒé‡è¿›è¡Œäº†åˆå§‹åŒ–ã€‚å®ƒå¯ä»¥ç›´æ¥ç”¨äºæ¨ç†å®ƒè®­ç»ƒè¿‡çš„ä»»åŠ¡ï¼Œä¹Ÿå¯ä»¥åœ¨æ–°ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒã€‚é€šè¿‡ä½¿ç”¨é¢„è®­ç»ƒçš„æƒé‡è¿›è¡Œè®­ç»ƒï¼Œç›¸æ¯”äºä»å¤´å¼€å§‹è®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥è¿…é€Ÿè·å¾—æ¯”è¾ƒå¥½çš„ç»“æœã€‚

æƒé‡å·²ä¸‹è½½å¹¶ç¼“å­˜åœ¨ç¼“å­˜æ–‡ä»¶å¤¹ä¸­ï¼ˆå› æ­¤ï¼Œæœªæ¥è°ƒç”¨Â `from_pretrained()`Â æ–¹æ³•çš„è°ƒç”¨å°†ä¸ä¼šé‡æ–°ä¸‹è½½å®ƒä»¬ï¼‰é»˜è®¤ä¸ºÂ `~/.cache/huggingface/transformers`Â ã€‚ä½ å¯ä»¥é€šè¿‡è®¾ç½®Â `HF_HOME`Â ç¯å¢ƒå˜é‡æ¥è‡ªå®šä¹‰ç¼“å­˜æ–‡ä»¶å¤¹ã€‚

åŠ è½½æ¨¡å‹çš„æ ‡è¯†ç¬¦å¯ä»¥æ˜¯ Model Hub ä¸Šä»»ä½•æ¨¡å‹çš„æ ‡ç­¾ï¼Œåªè¦å®ƒä¸ BERT æ¶æ„å…¼å®¹ã€‚å¯ç”¨çš„ BERT checkpoint çš„å®Œæ•´åˆ—è¡¨å¯ä»¥åœ¨Â [è¿™é‡Œ](https://huggingface.co/models?filter=bert)Â æ‰¾åˆ°ã€‚

### ä¿å­˜æ¨¡å‹

```python
model.save_pretrained("directory_on_my_computer")
```
### ä½¿ç”¨Transformeræ¨¡å‹è¿›è¡Œæ¨ç†
### ä½¿ç”¨å¼ é‡ä½œä¸ºæ¨¡å‹çš„è¾“å…¥

# [æ ‡è®°å™¨ï¼ˆTokenizerï¼‰](https://huggingface.co/learn/llm-course/zh-CN/chapter2/4)

## åŸºäºå•è¯ï¼ˆWord-basedï¼‰çš„ tokenization

## åŸºäºå­—ç¬¦ï¼ˆCharacter-basedï¼‰çš„ tokenization

## åŸºäºå­è¯ï¼ˆsubwordï¼‰çš„ tokenization

### è¿˜æœ‰æ›´å¤šï¼
- Byte-level BPEï¼Œç”¨äº GPT-2
- WordPieceï¼Œç”¨äº BERT
- SentencePiece or Unigramï¼Œç”¨äºå¤šä¸ªå¤šè¯­è¨€æ¨¡å‹

## åŠ è½½å’Œä¿å­˜
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```

```python
tokenizer.save_pretrained("directory_on_my_computer")
```

## ç¼–ç 
å°†æ–‡æœ¬ç¿»è¯‘æˆæ•°å­—è¢«ç§°ä¸ºç¼–ç ï¼ˆencodingï¼‰ã€‚ç¼–ç åˆ†ä¸¤æ­¥å®Œæˆï¼šåˆ†è¯ï¼Œç„¶åè½¬æ¢ä¸º inputs IDã€‚
### tokenization
### ä»tokensåˆ°inputs ID
## è§£ç 
è§£ç ï¼ˆDecodingï¼‰ æ­£å¥½ç›¸åï¼šä» inputs ID åˆ°ä¸€ä¸ªå­—ç¬¦ä¸²ã€‚è¿™ä»¥é€šè¿‡Â `decode()`Â æ–¹æ³•å®ç°ï¼š

```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```
# [å¤„ç†å¤šä¸ªåºåˆ—](https://huggingface.co/learn/llm-course/zh-CN/chapter2/5)

## æ¨¡å‹éœ€è¦ä¸€æ‰¹è¾“å…¥

## å¡«å……è¾“å…¥ï¼ˆPaddingï¼‰

## æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰å±‚
æ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰æ˜¯ä¸ inputs ID å¼ é‡å½¢çŠ¶å®Œå…¨ç›¸åŒçš„å¼ é‡ï¼Œç”¨ 0 å’Œ 1 å¡«å……ï¼š1 è¡¨ç¤ºåº”å…³æ³¨ç›¸åº”çš„ tokensï¼Œ0 è¡¨ç¤ºåº”å¿½ç•¥ç›¸åº”çš„ tokensï¼ˆå³ï¼Œå®ƒä»¬åº”è¢«æ¨¡å‹çš„æ³¨æ„åŠ›å±‚å¿½è§†ï¼‰ã€‚

## æ›´é•¿çš„å¥å­
å¯¹äº Transformers æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ¨¡å‹çš„åºåˆ—é•¿åº¦æ˜¯æœ‰é™çš„ã€‚å¤§å¤šæ•°æ¨¡å‹å¤„ç†å¤šè¾¾ 512 æˆ– 1024 ä¸ª çš„ tokens åºåˆ—ï¼Œå½“ä½¿ç”¨æ¨¡å‹å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ï¼Œä¼šå´©æºƒã€‚æ­¤é—®é¢˜æœ‰ä¸¤ç§è§£å†³æ–¹æ¡ˆï¼š

- ä½¿ç”¨æ”¯æŒæ›´é•¿åºåˆ—é•¿åº¦çš„æ¨¡å‹ã€‚
- æˆªæ–­ä½ çš„åºåˆ—ã€‚

ä¸åŒçš„æ¨¡å‹æ”¯æŒçš„åºåˆ—é•¿åº¦å„ä¸ç›¸åŒï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†éå¸¸é•¿çš„åºåˆ—ã€‚ä¾‹å¦‚Â [Longformer](https://huggingface.co/transformers/model_doc/longformer)Â å’ŒÂ [LED](https://huggingface.co/transformers/model_doc/led)Â ã€‚å¦‚æœä½ æ­£åœ¨å¤„ç†éœ€è¦éå¸¸é•¿åºåˆ—çš„ä»»åŠ¡ï¼Œæˆ‘ä»¬å»ºè®®ä½ è€ƒè™‘ä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚

å¦å¤–ï¼Œæˆ‘ä»¬å»ºè®®ä½ é€šè¿‡è®¾å®šÂ `max_sequence_length`Â å‚æ•°æ¥æˆªæ–­åºåˆ—ï¼š

```python
sequence = sequence[:max_sequence_length]
```


# æŠŠä»–ä»¬æ”¾åœ¨ä¸€èµ·

## ç»¼åˆåº”ç”¨

æ­£å¦‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„ä¸€äº›ä¾‹å­ä¸­çœ‹åˆ°çš„ï¼Œè¿™ä¸ªå‡½æ•°éå¸¸å¼ºå¤§ã€‚é¦–å…ˆï¼Œå®ƒå¯ä»¥å¯¹å•ä¸ªå¥å­è¿›è¡Œå¤„ç†ï¼š

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
```

å®ƒè¿˜ä¸€æ¬¡å¤„ç†å¤šä¸ªå¤šä¸ªï¼Œå¹¶ä¸” API çš„ç”¨æ³•å®Œå…¨ä¸€è‡´ï¼š

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

model_inputs = tokenizer(sequences)
```

å®ƒå¯ä»¥ä½¿ç”¨å¤šç§ä¸åŒçš„æ–¹æ³•å¯¹ç›®æ ‡è¿›è¡Œå¡«å……ï¼š
```python
# å°†å¥å­åºåˆ—å¡«å……åˆ°æœ€é•¿å¥å­çš„é•¿åº¦
model_inputs = tokenizer(sequences, padding="longest")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æ¨¡å‹çš„æœ€å¤§é•¿åº¦
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# å°†å¥å­åºåˆ—å¡«å……åˆ°æŒ‡å®šçš„æœ€å¤§é•¿åº¦
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
```

å®ƒè¿˜å¯ä»¥å¯¹åºåˆ—è¿›è¡Œæˆªæ–­ï¼š
```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# å°†æˆªæ–­æ¯”æ¨¡å‹æœ€å¤§é•¿åº¦é•¿çš„å¥å­åºåˆ—
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# å°†æˆªæ–­é•¿äºæŒ‡å®šæœ€å¤§é•¿åº¦çš„å¥å­åºåˆ—
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
```

`tokenizer`Â å¯¹è±¡å¯ä»¥å¤„ç†æŒ‡å®šæ¡†æ¶å¼ é‡çš„è½¬æ¢ï¼Œç„¶åå¯ä»¥ç›´æ¥å‘é€åˆ°æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸‹é¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å‘Šè¯‰ tokenizer è¿”å›ä¸åŒæ¡†æ¶çš„å¼ é‡ â€”â€”Â `"pt"`Â è¿”å› PyTorch å¼ é‡ï¼ŒÂ `"tf"`Â è¿”å› TensorFlow å¼ é‡ï¼Œè€ŒÂ `"np"`Â åˆ™è¿”å› NumPy æ•°ç»„ï¼š

```python
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# è¿”å› PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# è¿”å› TensorFlow tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="tf")

# è¿”å› NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
```


## [ç‰¹æ®Šçš„ tokens](https://huggingface.co/learn/llm-course/zh-CN/chapter2/6#%E7%89%B9%E6%AE%8A%E7%9A%84%20tokens)

å¦‚æœæˆ‘ä»¬çœ‹ä¸€ä¸‹ tokenizer è¿”å›çš„ inputs IDï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒä»¬ä¸ä¹‹å‰çš„ç•¥æœ‰ä¸åŒï¼š

```python
sequence = "I've been waiting for a HuggingFace course my whole life."

model_inputs = tokenizer(sequence)
print(model_inputs["input_ids"])

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```


```shell
[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]
[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]
```


å¥å­çš„å¼€å§‹å’Œç»“æŸåˆ†åˆ«å¢åŠ äº†ä¸€ä¸ª inputs IDã€‚æˆ‘ä»¬æ¥è§£ç ä¸Šè¿°çš„ä¸¤ä¸ª ID åºåˆ—ï¼Œçœ‹çœ‹æ˜¯æ€ä¹ˆå›äº‹ï¼š

```
print(tokenizer.decode(model_inputs["input_ids"]))
print(tokenizer.decode(ids))
```


```
"[CLS] i've been waiting for a huggingface course my whole life. [SEP]"
"i've been waiting for a huggingface course my whole life."
```

tokenizer åœ¨å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šå•è¯Â `[CLS]`Â ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šå•è¯Â `[SEP]`Â ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨äº†è¿™äº›å­—è¯ï¼Œæ‰€ä»¥ä¸ºäº†å¾—åˆ°ç›¸åŒçš„æ¨æ–­ç»“æœï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œæœ‰äº›æ¨¡å‹ä¸æ·»åŠ ç‰¹æ®Šå•è¯ï¼Œæˆ–è€…æ·»åŠ ä¸åŒçš„ç‰¹æ®Šå•è¯ï¼›æ¨¡å‹ä¹Ÿå¯èƒ½åªåœ¨å¼€å¤´æˆ–ç»“å°¾æ·»åŠ è¿™äº›ç‰¹æ®Šå•è¯ã€‚æ— è®ºå¦‚ä½•ï¼Œtokenizer çŸ¥é“å“ªäº›æ˜¯å¿…éœ€çš„ï¼Œå¹¶ä¼šä¸ºä½ å¤„ç†è¿™äº›é—®é¢˜ã€‚

## [å°ç»“ï¼šä» tokenizer åˆ°æ¨¡å‹](https://huggingface.co/learn/llm-course/zh-CN/chapter2/6#%E5%B0%8F%E7%BB%93%EF%BC%9A%E4%BB%8E%20tokenizer%20%E5%88%B0%E6%A8%A1%E5%9E%8B)

ç°åœ¨æˆ‘ä»¬å·²ç»çœ‹åˆ°Â `tokenizer`Â å¯¹è±¡åœ¨å¤„ç†æ–‡æœ¬æ—¶çš„æ‰€æœ‰æ­¥éª¤ï¼Œè®©æˆ‘ä»¬æœ€åå†çœ‹ä¸€æ¬¡å®ƒå¦‚ä½•é€šè¿‡ tokenizer API å¤„ç†å¤šä¸ªåºåˆ—ï¼ˆå¡«å……ï¼ï¼‰ï¼Œéå¸¸é•¿çš„åºåˆ—ï¼ˆæˆªæ–­ï¼ï¼‰ä»¥åŠå¤šç§ç±»å‹çš„å¼ é‡ï¼š

```python

import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")
output = model(**tokens)

```

# åŸºæœ¬ç”¨æ³•å®Œæˆ

- å­¦ä¹ äº† Transformers æ¨¡å‹çš„åŸºæœ¬æ„é€ å—ã€‚    
- äº†è§£äº† Tokenizer ç®¡é“çš„ç»„æˆã€‚
- äº†è§£äº†å¦‚ä½•åœ¨å®è·µä¸­ä½¿ç”¨ Transformers æ¨¡å‹ã€‚
- å­¦ä¹ äº†å¦‚ä½•åˆ©ç”¨ tokenizer å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„å¼ é‡ã€‚
- è®¾å®šäº† tokenizer å’Œæ¨¡å‹ï¼Œå¯ä»¥ä»è¾“å…¥çš„æ–‡æœ¬è·å–é¢„æµ‹çš„ç»“æœã€‚
- äº†è§£äº† inputs IDs çš„å±€é™æ€§ï¼Œå¹¶å­¦ä¹ äº†å…³äºæ³¨æ„åŠ›æ©ç ï¼ˆattention maskï¼‰çš„çŸ¥è¯†ã€‚
- è¯•ç”¨äº†çµæ´»ä¸”å¯é…ç½®çš„ Tokenizer æ–¹æ³•ã€‚