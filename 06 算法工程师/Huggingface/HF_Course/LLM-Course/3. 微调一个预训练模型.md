# æœ¬ç« ç®€ä»‹
- å¦‚ä½•ä»æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰åŠ è½½å¤§å‹æ•°æ®é›†
- å¦‚ä½•ä½¿ç”¨é«˜çº§çš„Â `Trainer`Â API å¾®è°ƒä¸€ä¸ªæ¨¡å‹
- å¦‚ä½•ä½¿ç”¨è‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹
- å¦‚ä½•åˆ©ç”¨ğŸ¤— Accelerate åº“åœ¨æ‰€æœ‰åˆ†å¸ƒå¼è®¾å¤‡ä¸Šè½»æ¾è¿è¡Œè‡ªå®šä¹‰è®­ç»ƒè¿‡ç¨‹

# [é¢„å¤„ç†æ•°æ®](https://huggingface.co/learn/llm-course/zh-CN/chapter3/2)

## å¤„ç†æ•°æ®


## ä»æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰åŠ è½½æ•°æ®é›†
æ¨¡å‹ä¸­å¿ƒï¼ˆhubï¼‰ä¸ä»…ä»…åŒ…å«æ¨¡å‹ï¼Œè¿˜æœ‰è®¸å¤šåˆ«çš„è¯­è¨€çš„æ•°æ®é›†ã€‚è®¿é—®Â [Datasets](https://huggingface.co/datasets)Â çš„é“¾æ¥å³å¯è¿›è¡Œæµè§ˆã€‚æˆ‘ä»¬å»ºè®®ä½ åœ¨å®Œæˆæœ¬èŠ‚çš„å­¦ä¹ åé˜…è¯»ä¸€ä¸‹Â [åŠ è½½å’Œå¤„ç†æ–°çš„æ•°æ®é›†](https://huggingface.co/docs/datasets/loading)Â è¿™ç¯‡æ–‡ç« ï¼Œè¿™ä¼šè®©ä½ å¯¹ huggingface çš„æ•°æ®é›†ç†è§£æ›´åŠ æ¸…æ™°ã€‚
ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ MRPC æ•°æ®é›†ä¸­çš„Â [GLUE åŸºå‡†æµ‹è¯•æ•°æ®é›†](https://gluebenchmark.com/)Â ä½œä¸ºæˆ‘ä»¬è®­ç»ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ï¼Œå®ƒæ˜¯æ„æˆ MRPC æ•°æ®é›†çš„ 10 ä¸ªæ•°æ®é›†ä¹‹ä¸€ï¼Œä½œä¸ºä¸€ä¸ªç”¨äºè¡¡é‡æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨ 10 ä¸ªä¸åŒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­æ€§èƒ½çš„å­¦æœ¯åŸºå‡†ã€‚

```python
from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets
```

```shell
DatasetDict({
    train: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 408
    })
    test: Dataset({
        features: ['sentence1', 'sentence2', 'label', 'idx'],
        num_rows: 1725
    })
})
```

## é¢„å¤„ç†æ•°æ®é›†

```python
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)
```

```python
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets
```


```shell
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
```
## åŠ¨æ€å¡«å……


# ä½¿ç”¨Trainer APIå¾®è°ƒæ¨¡å‹


## training

## è¯„ä¼°


# ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒ

## è®­ç»ƒå‰çš„å‡†å¤‡

## è®­ç»ƒå¾ªç¯

## è¯„ä¼°å¾ªç¯

##  ä½¿ç”¨ğŸ¤—AccelerateåŠ é€Ÿä½ çš„è®­ç»ƒå¾ªç¯


# [å¾®è°ƒï¼Œç« èŠ‚å›é¡¾ï¼](https://huggingface.co/learn/llm-course/zh-CN/chapter3/5)
# å¾®è°ƒï¼Œæ£€æŸ¥ï¼