# RAG

RAG（检索增强生成，**Retrieval-Augmented Generation**）是一种**生成式人工智能框架**，它通过在大型语言模型（LLM）的生成过程中**整合相关的外部、最新或特定领域的信息**，来增强其功能和准确性。

简单来说，RAG 就像是给 LLM 配备了一个“**知识库搜索助手**”。当用户提出问题时，LLM 不是仅仅依赖其预训练的静态知识来回答，而是先让搜索助手去外部知识库（如企业内部文档、实时网页、专业数据库等）中检索最相关的片段，然后将这些检索到的信息作为额外的上下文（Context）一起提供给 LLM，让它生成更准确、更及时、有事实依据的回答。

## 使用场景
1. 文档问答
2. 图谱问答
3. 工具召回
4. 示列召回

## 工作流程
1. 索引（indexing）/预处理阶段
	1. **数据(文档内容)提取和分块**
	2. 嵌入/向量化
	3. 存储（入库）
2. 检索-生成
	1. **查询向量化**   当用户输入一个查询（问题）时，系统使用与索引阶段相同的嵌入模型，将查询也转换为一个查询向量。
	2. **检索** 系统在向量数据库中搜索与查询向量“最相似”（通常是向量距离最近）的**Top-K** 个文档片段。这通过复杂的搜索算法（如近似最近邻搜索 ANN）实现
	3. 增强提示. 将检索到的相关文档片段和用户的原始查询结合起来，构建一个新的、增强的提示（**Context-Aware Prompt**）
	4. 生成    将增强后的提示输入给大型语言模型（LLM），LLM 根据这个新的、包含外部知识的上下文来生成最终的、准确的回答。

## 优化方向
- 检索优化 提高**召回率和相关性**

| **优化方式**                               | **描述**                                                  | **目的**                      |
| -------------------------------------- | ------------------------------------------------------- | --------------------------- |
| **细化分块（Chunking）策略**                   | 调整文档分块的大小、重叠度，或使用基于语义、标题、段落的智能分块。                       | 确保每个片段包含足够但不过多的上下文，以提高检索精度。 |
| **元数据过滤（Metadata Filtering）**          | 在检索时，不仅考虑文本相似性，还根据文档类型、日期、作者、权限等元数据进行预过滤。               | 提高针对性，确保检索到的信息是最新且合规的。      |
| **查询重写/扩展（Query Rewriting/Expansion）** | 使用一个小型 LLM 或特定模型来重写/扩展用户查询（例如，增加同义词或补充背景信息），以便更好地匹配知识库。 | 解决用户查询可能过于简洁或模糊的问题，提高检索召回率。 |
| **混合检索（Hybrid Search）**                | 同时使用向量搜索（语义相似性）和关键字搜索（如BM25）来检索文档。                      | 结合两种方法的优势，提高整体检索效果。         |
| **多跳/迭代检索**                            | 在初步检索和生成后，系统根据生成的中间结果进行多次检索，以寻找更深层次的支撑信息。               | 解决需要多步骤推理的复杂问题。             |
|                                        |                                                         |                             |
- 生成优化（提高质量和事实一致性）

| **优化方式**                       | **描述**                                                    | **目的**                         |
| ------------------------------ | --------------------------------------------------------- | ------------------------------ |
| **重新排序（Re-ranking）**           | 在检索到 Top-K 个文档片段后，使用一个更强大的模型（如交叉编码器）对这些片段进行二次排序，选出最相关的部分。 | 确保送入 LLM 的上下文是质量最高的，避免无关信息的干扰。 |
| **提示词工程（Prompt Engineering）**  | 精心设计给 LLM 的指令和提示，明确要求它“**仅依据提供的上下文回答**”，并要求引用来源。          | 更好地指导 LLM 使用检索到的上下文，并减少幻觉。     |
| **上下文压缩（Context Compression）** | 在将检索到的所有片段送入 LLM 之前，使用模型来压缩或总结这些上下文，仅保留关键信息。              | 适应 LLM 的上下文窗口限制，并提高推理效率。       |

- **检索阶段 (Retrieval) 的目标是：召回率 (Recall)**
    - 也就是“宁可错杀一千，不可放过一个”。
    - 这个阶段的 Query 需要**泛化**，包含更多的关键词、同义词，甚至要把缩写展开，为了尽可能多地从数据库里把相关内容捞出来。
- **重排序阶段 (Reranking) 的目标是：准确率 (Precision)**
    - 也就是“优中选优”。
    - 这个阶段的 Query 需要**精准**，最好还原成人类自然的语言逻辑，因为重排序模型（Cross-Encoder）擅长理解复杂的语义关系。