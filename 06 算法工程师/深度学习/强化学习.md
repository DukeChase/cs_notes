# ä¼ ç»Ÿå¼ºåŒ–å­¦ä¹ 

æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯**é€šè¿‡è¯•é”™æ¥å­¦ä¹ **ã€‚
æœºå™¨å­¦ä¹ çš„ä¸€ç§èŒƒå¼ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š  
ğŸ‘‰ **æ™ºèƒ½ä½“ï¼ˆAgentï¼‰é€šè¿‡ä¸ç¯å¢ƒï¼ˆEnvironmentï¼‰ä¸æ–­äº¤äº’ï¼Œæ ¹æ®å¾—åˆ°çš„å¥–åŠ±ï¼ˆRewardï¼‰æ¥å­¦ä¹ æœ€ä¼˜è¡Œä¸ºç­–ç•¥ï¼ˆPolicyï¼‰**ã€‚

## æ ¸å¿ƒè¦ç´ ï¼š

1. **æ™ºèƒ½ä½“ï¼ˆAgentï¼‰**ï¼šå­¦ä¹ å’Œåšå†³ç­–çš„ä¸»ä½“ã€‚
2. **ç¯å¢ƒï¼ˆEnvironmentï¼‰**ï¼šæ™ºèƒ½ä½“æ‰€å¤„çš„å¤–éƒ¨ä¸–ç•Œï¼Œä¼šæ ¹æ®æ™ºèƒ½ä½“çš„åŠ¨ä½œç»™å‡ºåé¦ˆã€‚
3. **çŠ¶æ€ï¼ˆStateï¼‰**ï¼šç¯å¢ƒåœ¨æŸä¸€æ—¶åˆ»çš„æƒ…å†µæè¿°ã€‚
4. **åŠ¨ä½œï¼ˆActionï¼‰**ï¼šæ™ºèƒ½ä½“å¯ä»¥åšå‡ºçš„è¡Œä¸ºã€‚
5. **å¥–åŠ±ï¼ˆRewardï¼‰**ï¼šç¯å¢ƒå¯¹æ™ºèƒ½ä½“åŠ¨ä½œçš„å³æ—¶åé¦ˆï¼ˆä¸€ä¸ªæ•°å€¼ï¼‰ã€‚
6. **ç­–ç•¥ï¼ˆPolicyï¼‰**ï¼šæ™ºèƒ½ä½“çš„è¡Œä¸ºå‡†åˆ™ï¼Œè§„å®šäº†åœ¨ä»€ä¹ˆçŠ¶æ€ä¸‹åº”è¯¥é‡‡å–ä»€ä¹ˆåŠ¨ä½œã€‚
7. **ä»·å€¼ï¼ˆValueï¼‰**ï¼šå¯¹é•¿æœŸç´¯ç§¯å¥–åŠ±çš„é¢„æµ‹ï¼Œä»£è¡¨ä¸€ä¸ªçŠ¶æ€æˆ–åŠ¨ä½œçš„â€œé•¿è¿œå¥½åâ€ã€‚


![](https://encrypted-tbn3.gstatic.com/licensed-image?q=tbn:ANd9GcS8x2u5eP1dm-SRo3t_8KlkaQF7vYX6487UqLivoWIzAPEyEHkbTWBqUEe5fpG1rzvK65iBV09HfOhPX089n_mAxCJ9LK64Cdy8yiD9KH4BQRBr1KY)

## å¼ºåŒ–å­¦ä¹ ä¸å…¶ä»–æœºå™¨å­¦ä¹ çš„åŒºåˆ«

| **ç‰¹æ€§**   | **ç›‘ç£å­¦ä¹  (Supervised)** | **å¼ºåŒ–å­¦ä¹  (Reinforcement)** |
| -------- | --------------------- | ------------------------ |
| **æ•°æ®æº**  | å·²ç»æ ‡è®°å¥½çš„æ•°æ®ï¼ˆæ­£ç¡®ç­”æ¡ˆï¼‰        | é€šè¿‡ä¸ç¯å¢ƒäº¤äº’äº§ç”Ÿçš„æ•°æ®             |
| **åé¦ˆæœºåˆ¶** | å‘Šè¯‰ä½ æ˜¯æˆ–é”™                | ç»™å‡ºåˆ†æ•°å€¼ï¼ˆå¥–åŠ±/æƒ©ç½šï¼‰             |
| **å†³ç­–æ€§è´¨** | ç‹¬ç«‹é¢„æµ‹ï¼ˆçœ‹å›¾è¯†çŒ«ï¼‰            | åºåˆ—å†³ç­–ï¼ˆä¸‹ä¸€æ­¥æ€ä¹ˆèµ°å½±å“æœªæ¥ï¼‰         |
| **å­¦ä¹ ç›®æ ‡** | å‡å°é¢„æµ‹è¯¯å·®                | æœ€å¤§åŒ–é•¿æœŸå›æŠ¥                  |

## ç›®æ ‡- > æœ€å¤§åŒ–é•¿æœŸç´¯è®¡å¥–åŠ±ï¼ˆExpected Cumulative Rewardï¼‰
æ‰¾åˆ°ä¸€ç§è¡ŒåŠ¨æ–¹æ¡ˆï¼ˆ$\pi$ï¼‰ï¼Œ ä½¿å¾—æ™ºèƒ½ä½“*agent*ä»ç°åœ¨å¼€å§‹åˆ°æœªæ¥æ‰€æœ‰èƒ½æ‹¿åˆ°çš„å¥–é‡‘ï¼ˆ**$r$**ï¼‰æ€»å’Œï¼Œåœ¨è€ƒè™‘äº†æœªæ¥çš„ä¸ç¡®å®šæ€§å’Œæ‰“æŠ˜ï¼ˆ**$\gamma$**ï¼‰ä¹‹åï¼Œå…¶å¹³å‡é¢„æœŸå€¼ï¼ˆ**$E$**ï¼‰è¾¾åˆ°æœ€å¤§ã€‚
$$r_t = R(s_t, a_t, s_{t+1})$$
$$\pi_{\max} E \left[ \sum_{t=0}^{\infty} \gamma^t r_t \right]$$

| **ç¬¦å·**                    | **åç§°**                     | **å«ä¹‰è§£é‡Š**                                                          |
| ------------------------- | -------------------------- | ----------------------------------------------------------------- |
| **$\pi$**                 | **ç­–ç•¥ (Policy)**            | æ™ºèƒ½ä½“çš„â€œå†³ç­–è“å›¾â€ã€‚å®ƒå®šä¹‰äº†åœ¨ç‰¹å®šçŠ¶æ€ä¸‹ï¼Œæ™ºèƒ½ä½“åº”è¯¥é‡‡å–ä»€ä¹ˆåŠ¨ä½œã€‚$\pi_{\max}$ è¡¨ç¤ºæˆ‘ä»¬è¦æ‰¾åˆ°ä¸€ä¸ª**æœ€ä¼˜ç­–ç•¥**ã€‚ |
| **$E$**                   | **æœŸæœ› (Expectation)**       | å› ä¸ºç¯å¢ƒå¾€å¾€å…·æœ‰éšæœºæ€§ï¼ˆæ¯”å¦‚æ·éª°å­æˆ–é£å‘å˜åŒ–ï¼‰ï¼Œæˆ‘ä»¬æ— æ³•ä¿è¯æ¯æ¬¡ç»“æœä¸€æ ·ï¼Œæ‰€ä»¥æˆ‘ä»¬è¦è®¡ç®—æ‰€æœ‰å¯èƒ½ç»“æœçš„**å¹³å‡é¢„æœŸå€¼**ã€‚     |
| **$\sum_{t=0}^{\infty}$** | **ç´¯åŠ å’Œ**                    | è¡¨ç¤ºä»æ—¶é—´æ­¥ $t=0$ å¼€å§‹ï¼Œä¸€ç›´åˆ°æ— ç©·è¿œçš„æœªæ¥ï¼Œå°†æ‰€æœ‰è·å¾—çš„å¥–åŠ±åŠ åœ¨ä¸€èµ·ã€‚                           |
| **$r_t$**                 | **å¥–åŠ± (Reward)**            | åœ¨æ—¶é—´æ­¥ $t$ æ—¶ï¼Œæ™ºèƒ½ä½“å› ä¸ºæ‰§è¡Œäº†æŸä¸ªåŠ¨ä½œè€Œä»ç¯å¢ƒè·å¾—çš„å³æ—¶åé¦ˆï¼ˆå¦‚å¾—åˆ†æˆ–æ‰£åˆ†ï¼‰ã€‚                       |
| **$\gamma$** (Gamma)      | **æŠ˜æ‰£å› å­ (Discount Factor)** | å–å€¼èŒƒå›´é€šå¸¸åœ¨ $[0, 1]$ ä¹‹é—´ã€‚å®ƒå†³å®šäº†æ™ºèƒ½ä½“**æœ‰å¤šçœ‹é‡æœªæ¥çš„å¥–åŠ±**ã€‚                         |

### å¥–åŠ±è®¡ç®—æ–¹å¼
1. ç¨€ç–å¥–åŠ±
2. å¯†é›†å¥–åŠ±
3. æƒ©ç½šé¡¹

## ç­–ç•¥ï¼ˆPolicyï¼‰

**ç­–ç•¥ Ï€**ï¼š
> ç»™å®šçŠ¶æ€ sï¼Œé€‰æ‹©åŠ¨ä½œ a çš„è§„åˆ™
- **ç¡®å®šæ€§ç­–ç•¥**ï¼š    $a = \pi(s)$
- **éšæœºç­–ç•¥**ï¼š    $\pi(a|s) = P(a \mid s)$

## å…¸å‹ç®—æ³•åˆ†ç±»

### **1ï¸âƒ£ åŸºäºä»·å€¼ï¼ˆValue-basedï¼‰**
å­¦ä¹ â€œè¿™ä¸ªçŠ¶æ€/åŠ¨ä½œå€¼ä¸å€¼é’±â€
- **Q-learning**
- **SARSA**
- **DQNï¼ˆDeep Q-Networkï¼‰**
---
### **2ï¸âƒ£ åŸºäºç­–ç•¥ï¼ˆPolicy-basedï¼‰**
ç›´æ¥å­¦ä¹ ç­–ç•¥
- **REINFORCE**
- **Policy Gradient**
---
### **3ï¸âƒ£ Actor-Criticï¼ˆæ··åˆï¼‰**
- Actorï¼šå­¦ç­–ç•¥
- Criticï¼šå­¦ä»·å€¼
å¸¸è§ç®—æ³•ï¼š
- **A2C / A3C**
- **PPO**
- **DDPG / SAC**
## ä¸»è¦æŒ‘æˆ˜
- å¥–åŠ±ç¨€ç–ï¼ˆSparse rewardsï¼‰ï¼šæ™ºèƒ½ä½“å¾ˆéš¾è·å¾—æœ‰æ•ˆåé¦ˆã€‚
- æ¢ç´¢ä¸åˆ©ç”¨çš„æƒè¡¡ï¼ˆExploration vs. Exploitationï¼‰ã€‚
- æ ·æœ¬æ•ˆç‡ä½ï¼ˆéœ€è¦å¤§é‡äº¤äº’ï¼‰ã€‚
- ç¯å¢ƒåŠ¨æ€å¤æ‚æˆ–éƒ¨åˆ†å¯è§‚æµ‹ã€‚

## å¼ºåŒ–å­¦ä¹ åº”ç”¨
æ“…é•¿å¤„ç†éœ€è¦**è¿ç»­å†³ç­–**çš„å¤æ‚ä»»åŠ¡ã€‚
- ğŸ® æ¸¸æˆï¼ˆAlphaGoã€Atariï¼‰
- ğŸš— è‡ªåŠ¨é©¾é©¶
- ğŸ¤– æœºå™¨äººæ§åˆ¶
- ğŸ“ˆ èµ„æºè°ƒåº¦ã€æ¨èç­–ç•¥
- ğŸ§  å¤§æ¨¡å‹å¯¹é½ï¼ˆRLHFï¼‰



# å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ 


[å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹  RLHF SFT PPO è¿™å‡ ä¸ªæ¦‚å¿µçš„é€»è¾‘å…³ç³»](https://chatgpt.com/c/695dcd75-c954-8325-a028-a7e4c473d708)

SFT æœ¬è´¨ä¸Šå°±æ˜¯å¼ºåŒ–å­¦ä¹ åŠ›çš„ Imitation Learning  / Behavior Cloning

## RLHF
**RLHF**ï¼ˆReinforcement Learning from Human Feedbackï¼Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰
## ç›®æ ‡

**è®©æ¨¡å‹â€œè¯´äººè¯ã€åšäººäº‹â€**ï¼Œå³å®ç° **äººç±»å¯¹é½ï¼ˆHuman Alignmentï¼‰**ã€‚
**RLHFçš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šå°†äººç±»æ¨¡ç³Šçš„â€œåå¥½â€è½¬åŒ–ä¸ºä¸€ä¸ªå¯ä¼˜åŒ–çš„æ•°å­¦ç›®æ ‡ã€‚**


> Hugging Face RLHF Blog

[huggingface-ChatGPT èƒŒåçš„â€œåŠŸè‡£â€â€”â€”RLHF æŠ€æœ¯è¯¦è§£](https://huggingface.co/blog/zh/rlhf)


RLHF ä¸‰æ­¥éª¤
1. SFT ï¼ˆSupervised Fine-Tuningï¼‰
2. RM å¥–åŠ±æ¨¡å‹
3. PPO

## SFT
$$
L_{SFT} = -\sum_{t}log\pi_\theta(y_t|x,y_{<t})
$$

## PPOï¼ˆProximal Policy Optimizationï¼‰

chat with gpt  [ä»€ä¹ˆæ˜¯å¼ºåŒ–å­¦ä¹ ](https://chatgpt.com/c/6948ff06-b378-8331-9536-b271f40b5f2c)

ç›®æ ‡   åœ¨æé«˜ç­–ç•¥è¡¨ç°çš„åŒæ—¶ï¼Œé™åˆ¶æ¯ä¸€æ¬¡æ›´æ–°ä¸è¦æ”¹å¾—å¤ªçŒ›ã€‚

å››ä¸ªæ¨¡å‹

|**æ¨¡å‹åç§°**|**è§’è‰² (Role)**|**ä½œç”¨**|**æ˜¯å¦æ›´æ–°å‚æ•°**|
|---|---|---|---|
|**Policy Model**|Actor (æ¼”å‘˜)|æ­£åœ¨è¢«ä¼˜åŒ–çš„ LLMï¼Œè´Ÿè´£ç”Ÿæˆå›å¤ã€‚|**æ˜¯**|
|**Value Model**|Critic (è¯„è®ºå®¶)|é¢„æµ‹å½“å‰çŠ¶æ€èƒ½è·å¾—çš„é•¿æœŸå›æŠ¥ï¼Œè¾…åŠ© Actor æ›´æ–°ã€‚|**æ˜¯**|
|**Reward Model**|Reward (å¥–åŠ±)|æ ¹æ®äººç±»åå¥½ç»™å›å¤æ‰“åˆ†ï¼ˆç”± RLHF ç¬¬äºŒæ­¥è®­ç»ƒå¥½ï¼‰ã€‚|å¦|
|**Reference Model**|Ref (å‚è€ƒ)|åˆå§‹çš„ SFT æ¨¡å‹ï¼Œç”¨æ¥é˜²æ­¢ Policy åç¦»å¤ªè¿œï¼ˆKL æ•£åº¦çº¦æŸï¼‰ã€‚|å¦|

| ç¬¦å·           | å«ä¹‰                           |
| ------------ | ---------------------------- |
| $s$          | çŠ¶æ€ï¼ˆç¯å¢ƒå½“å‰æƒ…å†µï¼‰                   |
| $a$          | åœ¨çŠ¶æ€ (s) ä¸‹é‡‡å–çš„åŠ¨ä½œ               |
| $\hat A$     | Advantageï¼ˆä¼˜åŠ¿ï¼‰ï¼Œè¡¨ç¤ºâ€œè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡å¥½å¤šå°‘â€ |
| $\mathbb{E}$ | å¯¹å¾ˆå¤šé‡‡æ ·å–å¹³å‡                     |
- $\pi_\theta(a|s)$ 



æ¦‚ç‡æ¯”ç‡ï¼ˆratioï¼‰
$$
r_tâ€‹(Î¸)= \frac{Ï€_Î¸â€‹(a_tâ€‹âˆ£s_tâ€‹)}{Ï€_{Î¸_{old}}â€‹â€‹(a_tâ€‹âˆ£s_t)â€‹)}â€‹â€‹
$$

| ç¬¦å·                    | å«ä¹‰         |
| --------------------- | ---------- |
| $t$                   | æ—¶é—´æ­¥        |
| $\theta$              | å½“å‰è¦æ›´æ–°çš„å‚æ•°   |
| $\theta_{\text{old}}$ | é‡‡æ ·æ•°æ®æ—¶çš„æ—§å‚æ•°  |
| $s_t$                 | ç¬¬ t æ­¥çš„çŠ¶æ€   |
| $a_t$                 | ç¬¬ t æ­¥é‡‡å–çš„åŠ¨ä½œ |

clip

$$
clip(rtâ€‹,1âˆ’Ïµ,1+Ïµ)
$$
æŠŠ rtr_trtâ€‹ **é™åˆ¶åœ¨** $[1-\epsilon,\,1+\epsilon]$ ä¹‹é—´

PPO
$$
L_{PPO}â€‹(Î¸)=E_tâ€‹[min(r_tâ€‹(Î¸)A^tâ€‹,clip(r_tâ€‹(Î¸),1âˆ’Ïµ,1+Ïµ)A^tâ€‹)]
$$

Advantage 
$$
\hat{A_t} = R_t-V(s_t)
$$
### KLæ•£åº¦
KL æ•£åº¦ï¼ˆKullbackâ€“Leibler Divergenceï¼‰å®šä¹‰ä¸ºï¼š

$$
KL(Ï€Î¸â€‹âˆ¥Ï€refâ€‹)=E_{aâˆ¼Ï€Î¸}â€‹â€‹[log \frac{Ï€_\theta(aâˆ£s)}{Ï€_{ref}â€‹(aâˆ£s)}â€‹]
$$

## DPO (Direct Preference Optimization)
**DPO**ï¼ˆDirect Preference Optimizationï¼Œ**ç›´æ¥åå¥½ä¼˜åŒ–**ï¼‰


## GRPO (Group Relative Policy Optimization)
**Group Relative Policy Optimization**ï¼ˆåˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰






TRL (Transformer Reinforcement Learning)

TRLX