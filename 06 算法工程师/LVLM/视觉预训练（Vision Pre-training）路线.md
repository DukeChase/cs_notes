目前视觉预训练（Vision Pre-training）已经从早期的“有监督时代”全面转向“自监督时代”，演化路径主要分为以下三大技术流派：

## 1. 掩码图像建模流派 (Masked Image Modeling, MIM)

这一流派将视觉任务“BERT化”，核心逻辑是：**遮住图像的一部分，让模型预测缺失的内容**。
- **MAE (Masked Autoencoders):** 由何恺明团队提出。采用非对称的编码器-解码器结构，仅处理未掩码的像素，通过极高的遮挡率（75%）强迫模型学习高层抽象特征。其预训练效率极高，是目前 ViT 架构 的标配。
- **BEiT:** 引入了类似 NLP 中“词表”的概念，将图像 Patch 映射为离散 Token，预测被遮挡位置的 Token ID。
- **iBOT:** 通过自蒸馏机制实现了“在线分词”，解决了 BEiT 需要预训练分词器的痛点。
## 2. 对比学习流派 (Contrastive Learning)
核心逻辑是：**将同一张图的不同增强版本拉近，将不同图的表示推远**，学习具有判别性的全局特征。
- **MoCo / SimCLR:** 经典的对比学习框架，奠定了利用正负样本对进行无监督学习的基础。
- **DINO / DINOv2:** Meta 提出的自蒸馏方法。DINOv2 不再依赖掩码，仅通过大规模数据的自监督对比，生成的特征直接用于下游任务（如深度估计、语义分割）时效果惊人，被称为视觉领域的 **基础模型（Foundation Model）**。
## 3. 多模态对齐流派 (Vision-Language Pre-training, VLP)
核心逻辑是：**将图片与文字在同一向量空间对齐**，使模型具备理解语义和图文关联的能力。

- **CLIP (OpenAI):** 划时代的作品。利用数亿对“图-文”对进行对比学习，使模型具备了强大的 **Zero-shot（零样本）** 迁移能力，是目前 Stable Diffusion 和 GPT-4V 等多模态模型的核心组件。
- **ALIGN / BLIP:** 进一步优化了图文对齐的效率和质量，BLIP 系列通过引入文本生成任务，增强了模型对图像细节的描述能力。

总结与趋势

|特性|MIM (如 MAE)|对比学习 (如 DINO)|多模态 (如 CLIP)|
|---|---|---|---|
|**学习目标**|局部重构/补全|全局语义一致性|图文语义对齐|
|**优势**|细节理解强，适合分割检测|特征判别力强，适合检索分类|零样本迁移，具备文字理解|
|**现状**|成为大模型 Backbone 的主流预训练方式|演进为高精度的特征提取器|演进为多模态大模型（VLM）的核心|

您是正在考虑为特定任务（如**工业检测**或**自动驾驶**）选择预训练模型，还是想了解如何在大规模**自定义数据集**上进行预训练？

[](https://labs.google.com/search/experiment/22)